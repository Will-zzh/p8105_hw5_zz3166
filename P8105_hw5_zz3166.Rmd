---
title: "P8105_hw5_zz3166"
author: "Zihan Zhao"
date: "2024-11-15"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(broom)
library(ggplot2)
library(dplyr)
```

# problem 1

## Introduction
Suppose you put *n* people in a room and want to know the probability that at least two people share a birthday. For simplicity, we'll assume there are no leap years (i.e., there are only 365 days) and that birthdays are uniformly distributed over the year.

In this analysis, we will:

- Write a function that simulates drawing birthdays for a group of *n* people and checks for shared birthdays.
- Run this simulation 10,000 times for each group size between 2 and 50.
- Compute and plot the probability that at least two people share a birthday as a function of group size.

## Simulation Function
First, we define a function to simulate one trial of the birthday problem for a group of size *n*.
```{r birthday-simulation-function}
# Function to simulate one trial
birthday_simulation <- function(n) {
  # Randomly draw birthdays for n people (numbers from 1 to 365)
  birthdays <- sample(1:365, n, replace = TRUE)
  # Check if there are any duplicate birthdays
  any(duplicated(birthdays))
}
```

## Running the Simulations
We run the simulation 10,000 times for each group size from 2 to 50.
```{r}

# Set the range of group sizes
group_sizes <- 2:50
# Number of simulations to run for each group size
num_simulations <- 10000
# Initialize a vector to store probabilities for each group size
probabilities <- numeric(length(group_sizes))

# Loop over each group size
for (i in seq_along(group_sizes)) {
  n <- group_sizes[i]
  # Run the simulation num_simulations times and store the results
  results <- replicate(num_simulations, birthday_simulation(n))
  # Calculate the probability of at least two people sharing a birthday
  probabilities[i] <- mean(results)
}
```

## Results
We plot the probability of at least two people sharing a birthday as a function of group size.
```{r}

# Plot the probability as a function of group size
plot(group_sizes, probabilities, type = 'b', pch = 19, col = 'blue',
     xlab = 'Group Size',
     ylab = 'Probability of Shared Birthday',
     main = 'Probability of Shared Birthdays vs. Group Size')
grid()

```
Comment on the results:The probability of at least two people sharing a birthday grows rapidly as the group size increases. Around a group size of 23, the probability exceeds 50%, which is a well-known result in probability theory. This phenomenon is called the Birthday Paradox: despite there being 365 possible birthdays, a surprisingly small group size is needed to have a better-than-even chance of a shared birthday. As the group size continues to increase,the probability approaches 1, meaning it becomes almost certain that at least two people share a birthday in larger groups.


# Problem 2

# Introduction
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected—in other words, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as **power**, and it depends on several factors, including the sample size, the effect size, and the error variance. In this analysis, we will conduct a simulation to explore power in a one-sample t-test.

# Simulation Setup
We will conduct simulations with the following parameters:

- Sample size (\( n \)): 30
- Standard deviation (\( \sigma \)): 5
- True mean (\( \mu \)): Varies from 0 to 6
- Number of simulations per \( \mu \) value: 5000
- Significance level (\( \alpha \)): 0.05

We will generate datasets from the model:

\[
x \sim \text{Normal}(\mu, \sigma)
\]

For each dataset, we will:

- Estimate the sample mean (\( \hat{\mu} \))
- Perform a one-sample t-test of \( H_0: \mu = 0 \)
- Record the p-value from the test

We will then analyze how the power of the test and the estimates of \( \hat{\mu} \) vary with the true value of \( \mu \).

### Parameters
```{r}
# Simulation parameters
n <- 30
sigma <- 5
mu_values <- 0:6
num_simulations <- 5000
alpha <- 0.05

```

## Simulation
We will perform the simulations across the specified values of \(\mu\).
```{r}
# Initialize an empty list to store results
simulation_results <- list()

# Loop over each true mean value
for (mu in mu_values) {
  # Initialize vectors to store estimates and p-values
  mu_hats <- numeric(num_simulations)
  p_values <- numeric(num_simulations)
  
  # Perform simulations
  for (i in 1:num_simulations) {
    # Generate a dataset
    x <- rnorm(n, mean = mu, sd = sigma)
    
    # Perform one-sample t-test against mu = 0
    t_test_result <- t.test(x, mu = 0)
    
    # Extract estimate and p-value using broom::tidy
    tidy_result <- tidy(t_test_result)
    mu_hats[i] <- tidy_result$estimate
    p_values[i] <- tidy_result$p.value
  }
  
  # Store results in the list
  simulation_results[[as.character(mu)]] <- data.frame(
    mu_hat = mu_hats,
    p_value = p_values,
    mu_true = mu
  )
}

```

## Results

### Power Analysis
We calculate the power for each μ value by computing the proportion of simulations where the null hypothesis was rejected.
```{r}
# Combine all results into one data frame
all_results <- bind_rows(simulation_results)

# Calculate power for each mu value
power_results <- all_results %>%
  group_by(mu_true) %>%
  summarise(
    power = mean(p_value < alpha)
  )

```

### Plot: Power vs. True Mean (\(\mu\))

```{r}
# Plot the power as a function of mu
ggplot(power_results, aes(x = mu_true, y = power)) +
  geom_line(color = 'blue') +
  geom_point(color = 'red') +
  labs(
    title = "Power vs. True Mean (\u03BC)",
    x = "True Mean (\u03BC)",
    y = "Power"
  ) +
  theme_minimal()

```

### Power vs. True Mean (\( \mu \))
In the first plot, which shows "Power vs. True Mean (\( \mu \))," we can see that the power of the test increases as the true value of \( \mu \) increases. Specifically:

- For \( \mu = 0 \), the power is approximately 0, as expected, since there's no effect.
- As \( \mu \) increases, the power increases, indicating a higher probability of correctly rejecting the null hypothesis when a true effect exists.
- At larger values of \( \mu \), such as \( \mu = 4 \) or higher, the power approaches 1, meaning the test is almost certain to reject the false null hypothesis.

**Association Between Effect Size and Power:**
The power of a statistical test is positively associated with the effect size (\( \mu \)). As the effect size increases, it becomes easier to detect the effect, leading to a higher power. This is evident from the plot, where the probability of rejecting the null hypothesis increases with larger values of \( \mu \). When the true effect size is small, the power is lower because the test has more difficulty distinguishing between noise and an actual effect.

## Estimation of \(\hat{\mu}\)

### Average Estimate of \(\hat{\mu}\) vs. True Mean (\(\mu\))
```{r}
# Calculate average estimate of mu_hat for each mu_true
mu_hat_results <- all_results %>%
  group_by(mu_true) %>%
  summarise(
    mean_mu_hat = mean(mu_hat)
  )

# Plot the average mu_hat vs. mu_true
ggplot(mu_hat_results, aes(x = mu_true, y = mean_mu_hat)) +
  geom_line(color = 'green') +
  geom_point(color = 'darkgreen') +
  labs(
    title = "Average Estimate of \u03BĈ vs. True Mean (\u03BC)",
    x = "True Mean (\u03BC)",
    y = "Average Estimate of \u03BĈ"
  ) +
  theme_minimal()

```

The second plot shows the "Average Estimate of \( \hat{\mu} \) vs. True Mean (\( \mu \))."

- As \( \mu \) increases, the average estimate of \( \hat{\mu} \) increases linearly and is approximately equal to the true mean.
- This demonstrates that the sample mean (\( \hat{\mu} \)) is an unbiased estimator of the true mean (\( \mu \)).

### Average Estimate of \(\hat{\mu}\) for Rejected Null Hypotheses
```{r}
# Calculate average mu_hat only for simulations where null was rejected
mu_hat_rejected_results <- all_results %>%
  filter(p_value < alpha) %>%
  group_by(mu_true) %>%
  summarise(
    mean_mu_hat_rejected = mean(mu_hat)
  )

# Merge the two datasets
mu_hat_combined <- mu_hat_results %>%
  left_join(mu_hat_rejected_results, by = "mu_true")

# Plot both averages
ggplot(mu_hat_combined, aes(x = mu_true)) +
  geom_line(aes(y = mean_mu_hat, color = 'All Samples')) +
  geom_point(aes(y = mean_mu_hat, color = 'All Samples')) +
  geom_line(aes(y = mean_mu_hat_rejected, color = 'Rejected Null')) +
  geom_point(aes(y = mean_mu_hat_rejected, color = 'Rejected Null')) +
  labs(
    title = "Average Estimate of \u03BĈ vs. True Mean (\u03BC)",
    x = "True Mean (\u03BC)",
    y = "Average Estimate of \u03BĈ",
    color = "Samples"
  ) +
  theme_minimal()

```

The third plot shows the "Average Estimate of \( \hat{\mu} \) vs. True Mean (\( \mu \))" for both all samples and the subset of samples where the null hypothesis was rejected.

- The blue line represents the average estimate of \( \hat{\mu} \) for rejected null hypotheses, while the red line represents the average for all samples.
- For the rejected null hypotheses, the average estimate of \( \hat{\mu} \) is generally higher compared to all samples, particularly for smaller values of \( \mu \).

**Is the Sample Average of \( \hat{\mu} \) Across Tests Where the Null Is Rejected Approximately Equal to the True Value of \( \mu \)?**

- **No,** the sample average of \( \hat{\mu} \) across tests where the null is rejected is not always approximately equal to the true value of \( \mu \). This is because there is a selection bias in considering only the samples where the null hypothesis was rejected. The estimates of \( \hat{\mu} \) in this subset are typically skewed upwards because we are more likely to reject the null hypothesis in cases where the sample mean is farther from 0 (the hypothesized null value). This phenomenon is often referred to as "regression toward the mean" or selection bias in hypothesis testing.

## Summary
- **Power vs. True Mean:** The power of the test increases with the effect size, \( \mu \). When the true mean is larger, it is easier to reject the null hypothesis, leading to higher power.
- **Average Estimate of \( \hat{\mu} \):** The average estimate of \( \hat{\mu} \) is generally unbiased and closely matches the true mean \( \mu \) across all samples.
- **Rejected Null Hypotheses:** When considering only rejected null hypotheses, the average estimate of \( \hat{\mu} \) is higher than for all samples, due to selection bias. This bias occurs because we are more likely to reject the null when \( \hat{\mu} \) is farther from the hypothesized value.

These conclusions provide insights into the behavior of hypothesis testing, demonstrating how power is influenced by effect size and how selection bias can impact our interpretation of estimates when only considering significant results.












